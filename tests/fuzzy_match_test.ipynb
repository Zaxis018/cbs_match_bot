{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gregorian datetime: 2007-07-14\n"
     ]
    }
   ],
   "source": [
    "import nepali_datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Example Nepali date string\n",
    "nepali_date_str = '2064-03-30'  # Replace with a valid Nepali date\n",
    "\n",
    "# Split the Nepali date string into year, month, and day\n",
    "year, month, day = map(int, nepali_date_str.split('-'))\n",
    "\n",
    "# Convert to Nepali date object\n",
    "nepali_date_obj = nepali_datetime.date(year, month, day)\n",
    "\n",
    "# Convert the Nepali date object to Gregorian datetime\n",
    "gregorian_datetime_obj = nepali_date_obj.to_datetime_date()\n",
    "\n",
    "# Print the result\n",
    "print(\"Gregorian datetime:\", gregorian_datetime_obj)\n",
    "\n",
    "def convert_bs_to_ad(nepali_date):\n",
    "    nepali_date_str = str(nepali_date)\n",
    "    \n",
    "    year, month, day = map(int, nepali_date_str.split('-'))\n",
    "    nepali_date_obj = nepali_datetime.date(year, month, day)\n",
    "    gregorian_datetime_obj = nepali_date_obj.to_datetime_date()\n",
    "    return gregorian_datetime_obj\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# oracle_df = pd.read_excel(r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\test\\10K_cbs_view.xlsx')\n",
    "# source_ticket_df = pd.read_excel(r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\test\\letter_actions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIF_ID</th>\n",
       "      <th>CUST_FIRST_NAME</th>\n",
       "      <th>CUST_MIDDLE_NAME</th>\n",
       "      <th>CUST_LAST_NAME</th>\n",
       "      <th>FATHERS_NAME</th>\n",
       "      <th>GRAND_FATHERS_NAME</th>\n",
       "      <th>CUST_DOB</th>\n",
       "      <th>CUST_CIF_OPN_DATE</th>\n",
       "      <th>CTZ_NUMBER</th>\n",
       "      <th>CTZ_ISSUE_DATE</th>\n",
       "      <th>CTZ_ISSUED_DISTRICT</th>\n",
       "      <th>NID_NUMBER</th>\n",
       "      <th>NID_ISSUEDDISTRICT</th>\n",
       "      <th>PERM_ADDDRES</th>\n",
       "      <th>ACCT_NUMBER</th>\n",
       "      <th>ACCT_NAME</th>\n",
       "      <th>ACCT_OPN_DATE</th>\n",
       "      <th>ACCT_STATUS</th>\n",
       "      <th>FREZ_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R00000001</td>\n",
       "      <td>LAXMAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHRESTHA</td>\n",
       "      <td>DABAL BAHADUR SHRESTHA</td>\n",
       "      <td>BHABANI SANKAR SHRESTHA</td>\n",
       "      <td>1949-06-16 00:00:00</td>\n",
       "      <td>2007-09-17</td>\n",
       "      <td>988</td>\n",
       "      <td>1973-09-17 12:00:00</td>\n",
       "      <td>KATHMANDU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KATHMANDU</td>\n",
       "      <td>10000000100001</td>\n",
       "      <td>LAXMAN SHRESTHA</td>\n",
       "      <td>2007-09-17</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R00000002</td>\n",
       "      <td>SANTOSH</td>\n",
       "      <td>KUMAR</td>\n",
       "      <td>LAMA</td>\n",
       "      <td>PASANG LAMA</td>\n",
       "      <td>BIR BAHADUR LAMA</td>\n",
       "      <td>1955-02-21 00:00:00</td>\n",
       "      <td>2001-04-03</td>\n",
       "      <td>7950</td>\n",
       "      <td>1980-02-01 12:00:00</td>\n",
       "      <td>KATHMANDU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KATHMANDU</td>\n",
       "      <td>10000000200001</td>\n",
       "      <td>SANTOSH K LAMA</td>\n",
       "      <td>2001-04-03</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R00000003</td>\n",
       "      <td>PHURBA</td>\n",
       "      <td>WANGEL</td>\n",
       "      <td>LAMA</td>\n",
       "      <td>RINJE WANGU LAMA</td>\n",
       "      <td>SANGE LAMA</td>\n",
       "      <td>1963-05-02 00:00:00</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>4345</td>\n",
       "      <td>1989-07-12 12:00:00</td>\n",
       "      <td>SUNSARI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUNSARI</td>\n",
       "      <td>10000000300001</td>\n",
       "      <td>PHURBA WANGEL LAMA</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R00000006</td>\n",
       "      <td>MAHESH</td>\n",
       "      <td>PRASAD</td>\n",
       "      <td>BHATTARAI</td>\n",
       "      <td>SANAT PRASAD BHATTARAI</td>\n",
       "      <td>MAYA NATH BHATTARAI</td>\n",
       "      <td>1960-04-15 00:00:00</td>\n",
       "      <td>2004-05-20</td>\n",
       "      <td>8140/18294</td>\n",
       "      <td>1978-06-23 12:00:00</td>\n",
       "      <td>KOSHI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KOSHI</td>\n",
       "      <td>10000000600001</td>\n",
       "      <td>MAHESH PRASAD BHATTARAI</td>\n",
       "      <td>2004-05-20</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R00000008</td>\n",
       "      <td>SEETA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GURUNG</td>\n",
       "      <td>KRISHNA BAHADUR GHALE</td>\n",
       "      <td>MAN BAHADUR GHALE</td>\n",
       "      <td>1948-10-03 00:00:00</td>\n",
       "      <td>2001-04-03</td>\n",
       "      <td>2089/1998</td>\n",
       "      <td>2007-02-12 12:00:00</td>\n",
       "      <td>KATHMANDU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GANDAKI</td>\n",
       "      <td>10000000800001</td>\n",
       "      <td>SEETA GURUNG</td>\n",
       "      <td>2001-04-03</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CIF_ID CUST_FIRST_NAME CUST_MIDDLE_NAME CUST_LAST_NAME  \\\n",
       "0  R00000001          LAXMAN              NaN       SHRESTHA   \n",
       "1  R00000002         SANTOSH            KUMAR           LAMA   \n",
       "2  R00000003          PHURBA           WANGEL           LAMA   \n",
       "3  R00000006          MAHESH           PRASAD      BHATTARAI   \n",
       "4  R00000008           SEETA              NaN         GURUNG   \n",
       "\n",
       "             FATHERS_NAME       GRAND_FATHERS_NAME             CUST_DOB  \\\n",
       "0  DABAL BAHADUR SHRESTHA  BHABANI SANKAR SHRESTHA  1949-06-16 00:00:00   \n",
       "1             PASANG LAMA         BIR BAHADUR LAMA  1955-02-21 00:00:00   \n",
       "2        RINJE WANGU LAMA               SANGE LAMA  1963-05-02 00:00:00   \n",
       "3  SANAT PRASAD BHATTARAI      MAYA NATH BHATTARAI  1960-04-15 00:00:00   \n",
       "4   KRISHNA BAHADUR GHALE        MAN BAHADUR GHALE  1948-10-03 00:00:00   \n",
       "\n",
       "  CUST_CIF_OPN_DATE  CTZ_NUMBER      CTZ_ISSUE_DATE CTZ_ISSUED_DISTRICT  \\\n",
       "0        2007-09-17         988 1973-09-17 12:00:00           KATHMANDU   \n",
       "1        2001-04-03        7950 1980-02-01 12:00:00           KATHMANDU   \n",
       "2        2007-10-01        4345 1989-07-12 12:00:00             SUNSARI   \n",
       "3        2004-05-20  8140/18294 1978-06-23 12:00:00               KOSHI   \n",
       "4        2001-04-03   2089/1998 2007-02-12 12:00:00           KATHMANDU   \n",
       "\n",
       "   NID_NUMBER  NID_ISSUEDDISTRICT PERM_ADDDRES     ACCT_NUMBER  \\\n",
       "0         NaN                 NaN    KATHMANDU  10000000100001   \n",
       "1         NaN                 NaN    KATHMANDU  10000000200001   \n",
       "2         NaN                 NaN      SUNSARI  10000000300001   \n",
       "3         NaN                 NaN        KOSHI  10000000600001   \n",
       "4         NaN                 NaN      GANDAKI  10000000800001   \n",
       "\n",
       "                 ACCT_NAME ACCT_OPN_DATE ACCT_STATUS FREZ_CODE  \n",
       "0          LAXMAN SHRESTHA    2007-09-17           A         D  \n",
       "1           SANTOSH K LAMA    2001-04-03           A            \n",
       "2       PHURBA WANGEL LAMA    2007-10-01           A            \n",
       "3  MAHESH PRASAD BHATTARAI    2004-05-20           A            \n",
       "4             SEETA GURUNG    2001-04-03           D         D  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>letter_head</th>\n",
       "      <th>chalani_no</th>\n",
       "      <th>letter_date</th>\n",
       "      <th>institution</th>\n",
       "      <th>name</th>\n",
       "      <th>account_no</th>\n",
       "      <th>address</th>\n",
       "      <th>citizenship_no</th>\n",
       "      <th>fathers_name</th>\n",
       "      <th>grandfathers_name</th>\n",
       "      <th>issue_date</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_address</th>\n",
       "      <th>company_pan_no</th>\n",
       "      <th>company_registration_no</th>\n",
       "      <th>bank_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regarding freezing bank accounts</td>\n",
       "      <td>Land Management, Cooperatives and Poverty Alle...</td>\n",
       "      <td>538-L9</td>\n",
       "      <td>2081-08-24</td>\n",
       "      <td>Nepal Rastra Bank</td>\n",
       "      <td>Rajan Kishori K.C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kathmandu, Pyuthan</td>\n",
       "      <td>531016/339</td>\n",
       "      <td>Ravindra K.C.</td>\n",
       "      <td>Megh Bahadur K.C.</td>\n",
       "      <td>2064-03-32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regarding freezing bank accounts</td>\n",
       "      <td>Land Management, Cooperatives and Poverty Alle...</td>\n",
       "      <td>538-L9</td>\n",
       "      <td>2081-08-24</td>\n",
       "      <td>Nepal Rastra Bank</td>\n",
       "      <td>Rajendra Pradhan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>5360</td>\n",
       "      <td>Hasta Bahadur Pradhan</td>\n",
       "      <td>Ganesh Bahadur Pradhan</td>\n",
       "      <td>2043-05-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regarding freezing bank accounts</td>\n",
       "      <td>Land Management, Cooperatives and Poverty Alle...</td>\n",
       "      <td>538-L9</td>\n",
       "      <td>2081-08-24</td>\n",
       "      <td>Nepal Rastra Bank</td>\n",
       "      <td>Saman Maharjan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lalitpur</td>\n",
       "      <td>38472</td>\n",
       "      <td>Krishna Maharjan</td>\n",
       "      <td>Silal Maharjan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Regarding freezing bank accounts</td>\n",
       "      <td>Land Management, Cooperatives and Poverty Alle...</td>\n",
       "      <td>538-L9</td>\n",
       "      <td>2081-08-24</td>\n",
       "      <td>Nepal Rastra Bank</td>\n",
       "      <td>Subhadra Shrestha Nakarmi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lalitpur</td>\n",
       "      <td>10/1155</td>\n",
       "      <td>Ram Krishna Shrestha</td>\n",
       "      <td>Hari Bahadur Shrestha</td>\n",
       "      <td>2044-12-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regarding freezing bank accounts</td>\n",
       "      <td>Land Management, Cooperatives and Poverty Alle...</td>\n",
       "      <td>538-L9</td>\n",
       "      <td>2081-08-24</td>\n",
       "      <td>Nepal Rastra Bank</td>\n",
       "      <td>Sami Darai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chitwan</td>\n",
       "      <td>13/03870013</td>\n",
       "      <td>Harka Bahadur Darai</td>\n",
       "      <td>Aitaram Darai</td>\n",
       "      <td>2036-06-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            subject  \\\n",
       "0  Regarding freezing bank accounts   \n",
       "1  Regarding freezing bank accounts   \n",
       "2  Regarding freezing bank accounts   \n",
       "3  Regarding freezing bank accounts   \n",
       "4  Regarding freezing bank accounts   \n",
       "\n",
       "                                         letter_head chalani_no letter_date  \\\n",
       "0  Land Management, Cooperatives and Poverty Alle...     538-L9  2081-08-24   \n",
       "1  Land Management, Cooperatives and Poverty Alle...     538-L9  2081-08-24   \n",
       "2  Land Management, Cooperatives and Poverty Alle...     538-L9  2081-08-24   \n",
       "3  Land Management, Cooperatives and Poverty Alle...     538-L9  2081-08-24   \n",
       "4  Land Management, Cooperatives and Poverty Alle...     538-L9  2081-08-24   \n",
       "\n",
       "         institution                       name account_no  \\\n",
       "0  Nepal Rastra Bank         Rajan Kishori K.C.        NaN   \n",
       "1  Nepal Rastra Bank           Rajendra Pradhan        NaN   \n",
       "2  Nepal Rastra Bank             Saman Maharjan        NaN   \n",
       "3  Nepal Rastra Bank  Subhadra Shrestha Nakarmi        NaN   \n",
       "4  Nepal Rastra Bank                 Sami Darai        NaN   \n",
       "\n",
       "              address citizenship_no           fathers_name  \\\n",
       "0  Kathmandu, Pyuthan     531016/339          Ravindra K.C.   \n",
       "1           Kathmandu           5360  Hasta Bahadur Pradhan   \n",
       "2            Lalitpur          38472       Krishna Maharjan   \n",
       "3            Lalitpur        10/1155   Ram Krishna Shrestha   \n",
       "4             Chitwan    13/03870013    Harka Bahadur Darai   \n",
       "\n",
       "        grandfathers_name  issue_date  company_name  company_address  \\\n",
       "0       Megh Bahadur K.C.  2064-03-32           NaN              NaN   \n",
       "1  Ganesh Bahadur Pradhan  2043-05-08           NaN              NaN   \n",
       "2          Silal Maharjan         NaN           NaN              NaN   \n",
       "3   Hari Bahadur Shrestha  2044-12-18           NaN              NaN   \n",
       "4           Aitaram Darai  2036-06-29           NaN              NaN   \n",
       "\n",
       "   company_pan_no  company_registration_no  bank_name  \n",
       "0             NaN                      NaN        NaN  \n",
       "1             NaN                      NaN        NaN  \n",
       "2             NaN                      NaN        NaN  \n",
       "3             NaN                      NaN        NaN  \n",
       "4             NaN                      NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ticket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIF_ID', 'CUST_FIRST_NAME', 'CUST_MIDDLE_NAME', 'CUST_LAST_NAME',\n",
       "       'FATHERS_NAME', 'GRAND_FATHERS_NAME', 'CUST_DOB', 'CUST_CIF_OPN_DATE',\n",
       "       'CTZ_NUMBER', 'CTZ_ISSUE_DATE', 'CTZ_ISSUED_DISTRICT', 'NID_NUMBER',\n",
       "       'NID_ISSUEDDISTRICT', 'PERM_ADDDRES', 'ACCT_NUMBER', 'ACCT_NAME',\n",
       "       'ACCT_OPN_DATE', 'ACCT_STATUS', 'FREZ_CODE', 'total_score', 'criterion',\n",
       "       'name_score', 'fathers_name_score', 'grandfathers_name_score',\n",
       "       'citizenship_no_score', 'source_subject', 'source_letter_head',\n",
       "       'source_chalani_no', 'source_letter_date', 'source_institution',\n",
       "       'source_name', 'source_account_no', 'source_address',\n",
       "       'source_citizenship_no', 'source_fathers_name',\n",
       "       'source_grandfathers_name', 'source_issue_date', 'source_company_name',\n",
       "       'source_company_address', 'source_company_pan_no',\n",
       "       'source_company_registration_no', 'source_bank_name',\n",
       "       'source_entity_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_excel = r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\test\\matched\\ticket_10_Narayan_Prasad_Gautam.xlsx'\n",
    "\n",
    "df = pd.read_excel(matched_excel)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nepali_datetime\n",
    "from datetime import datetime\n",
    "from rapidfuzz import fuzz, process, utils\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "# from qrlib.QRComponent import QRComponent\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def convert_bs_to_ad(nepali_date):\n",
    "    nepali_date_str = str(nepali_date)\n",
    "    \n",
    "    year, month, day = map(int, nepali_date_str.split('-'))\n",
    "    nepali_date_obj = nepali_datetime.date(year, month, day)\n",
    "    gregorian_datetime_obj = nepali_date_obj.to_datetime_date()\n",
    "    return gregorian_datetime_obj\n",
    "\n",
    "class FuzzyMatcherComponent():\n",
    "    \"\"\"Enhanced component for fuzzy matching with dynamic weightage criteria based on available fields\"\"\"\n",
    "\n",
    "    def __init__(self, weightage_file: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the fuzzy matcher with weightage distribution file\n",
    "        \n",
    "        Args:\n",
    "            weightage_file: Optional path to the Excel file containing weightage distributions\n",
    "        \"\"\"\n",
    "        self.weightage_file = weightage_file\n",
    "        self.weightage_distributions = self._create_default_weightage()\n",
    "        \n",
    "    def read_weights(self, weights_excel):\n",
    "        weights_excel_df = pd.read_excel(r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\WeightageSheet.xlsx')\n",
    "        return weights_excel_df\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _calculate_text_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate similarity ratio between two text fields using RapidFuzz\"\"\"\n",
    "        if pd.isna(text1) or pd.isna(text2) or text1 is None or text2 is None:\n",
    "            return 0.0\n",
    "\n",
    "        text1 = self._preprocess_text(text1)\n",
    "        text2 = self._preprocess_text(text2)\n",
    "        \n",
    "        if not text1 or not text2:\n",
    "            return 0.0\n",
    "\n",
    "        standard_ratio = fuzz.ratio(text1, text2) / 100.0           \n",
    "        return standard_ratio\n",
    "\n",
    "    def _calculate_date_similarity(self, date1: str, date2: str) -> float:\n",
    "        \"\"\"Calculate similarity between two dates\"\"\"\n",
    "        if pd.isna(date1) or pd.isna(date2) or date1 is None or date2 is None:\n",
    "            return 0.0\n",
    "            \n",
    "        try:\n",
    "            d1 = pd.to_datetime(date1)\n",
    "            d2 = pd.to_datetime(date2)\n",
    "            logger.info(f\"Date comparision: CBS==>{d1}----- Ticket===>{d2}\")\n",
    "            \n",
    "            days_diff = abs((d1 - d2).days)\n",
    "            \n",
    "            return max(0, 1 - (days_diff / 365))\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def _get_available_criteria(self, record: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Determine which matching criteria are available in the input record\"\"\"\n",
    "        available_criteria = []\n",
    "        \n",
    "        if 'name' in record and not pd.isna(record['name']) and record['name']:\n",
    "            available_criteria.append('name')\n",
    "            \n",
    "        if 'pan_number' in record and not pd.isna(record['pan_number']) and record['pan_number']:\n",
    "            available_criteria.append('pan_number')\n",
    "            \n",
    "        if 'registration_number' in record and not pd.isna(record['registration_number']) and record['registration_number']:\n",
    "            available_criteria.append('registration_number')\n",
    "            \n",
    "        if 'fathers_name' in record and not pd.isna(record['fathers_name']) and record['fathers_name']:\n",
    "            available_criteria.append('fathers_name')\n",
    "            \n",
    "        if 'dob' in record and not pd.isna(record['dob']) and record['dob']:\n",
    "            available_criteria.append('dob')\n",
    "            \n",
    "        if 'citizenship_no' in record and not pd.isna(record['citizenship_no']) and record['citizenship_no']:\n",
    "            available_criteria.append('citizenship_no')\n",
    "\n",
    "        if 'account_no' in record and not pd.isna(record['account_no']) and record['account_no']:\n",
    "            available_criteria.append('account_no')\n",
    "            \n",
    "        return available_criteria\n",
    "\n",
    "    def _determine_entity_type(self, record: Dict[str, Any]) -> str:\n",
    "        \"\"\"Determine if the record is for an individual, institution, or account\"\"\"\n",
    "        entity_type = record.get('entity_type')\n",
    "        \n",
    "        if not entity_type:\n",
    "            if 'account_no' in record and not pd.isna(record['account_no']) and record['account_no']:\n",
    "                return 'account_no'\n",
    "                \n",
    "            if ('registration_number' in record and not pd.isna(record['registration_number']) and record['registration_number']) or \\\n",
    "            ('pan_number' in record and not pd.isna(record['pan_number']) and record['pan_number']):\n",
    "                return 'institutional'\n",
    "                \n",
    "            return 'individual'\n",
    "            \n",
    "        return entity_type\n",
    "\n",
    "\n",
    "    def _get_weightage_config(self, entity_type: str, available_criteria: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get the appropriate weightage configuration based on entity type and available criteria\n",
    "        \n",
    "        Args:\n",
    "            entity_type: Type of entity ('individual', 'institutional', 'account_no')\n",
    "            available_criteria: List of available criteria fields\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping field names to weights\n",
    "        \"\"\"\n",
    "        if entity_type not in self.weightage_distributions:\n",
    "            logger.warning(f\"Unknown entity type: {entity_type}, falling back to individual weightage\")\n",
    "            entity_type = 'individual'\n",
    "        \n",
    "        print(f\"Entity type is {entity_type}\")\n",
    "        entity_configs = self.weightage_distributions[entity_type]\n",
    "        \n",
    "        if entity_type == 'institutional' or entity_type == 'institution':\n",
    "            has_name = 'name' in available_criteria\n",
    "            has_pan = 'pan_number' in available_criteria\n",
    "            has_reg = 'registration_number' in available_criteria\n",
    "            \n",
    "            if has_name and has_pan:\n",
    "                return entity_configs['name_pan']\n",
    "            elif has_name and has_reg:\n",
    "                return entity_configs['reg']\n",
    "            elif has_pan:\n",
    "                return entity_configs['pan']\n",
    "            # Fall back to equal weightage if no matching configuration\n",
    "                \n",
    "        elif entity_type == 'account_no':\n",
    "            has_account = 'account_no' in available_criteria\n",
    "            has_name = 'name' in available_criteria\n",
    "            \n",
    "            if has_account and has_name:\n",
    "                return entity_configs['account_no_name']\n",
    "            elif has_account:\n",
    "                return entity_configs['account_no']\n",
    "                    \n",
    "        else:  # individual\n",
    "            has_name = 'name' in available_criteria\n",
    "            has_citizenship = 'citizenship_no' in available_criteria\n",
    "            has_fathers = 'fathers_name' in available_criteria\n",
    "            has_dob = 'dob' in available_criteria\n",
    "            \n",
    "            # Count the number of available criteria\n",
    "            criteria_count = sum([has_name, has_citizenship, has_fathers, has_dob])\n",
    "            \n",
    "            if criteria_count == 4:\n",
    "                return entity_configs['all_info']\n",
    "            elif criteria_count == 3:\n",
    "                if has_name and has_citizenship and has_fathers:\n",
    "                    return entity_configs['any_three_info']['name_citizenship_no_fathers_name']\n",
    "                elif has_name and has_citizenship and has_dob:\n",
    "                    return entity_configs['any_three_info']['name_citizenship_no_dob']\n",
    "                elif has_name and has_fathers and has_dob:\n",
    "                    return entity_configs['any_three_info']['name_fathers_name_dob']\n",
    "                elif has_citizenship and has_fathers and has_dob:\n",
    "                    return entity_configs['any_three_info']['citizenship_no_fathers_name_dob']\n",
    "            elif criteria_count == 2:\n",
    "                if has_name and has_citizenship:\n",
    "                    return entity_configs['any_two_info']['name_citizenship_no']\n",
    "                elif has_name and has_fathers:\n",
    "                    return entity_configs['any_two_info']['name_fathers_name']\n",
    "                elif has_name and has_dob:\n",
    "                    return entity_configs['any_two_info']['name_dob']\n",
    "                elif has_citizenship and has_fathers:\n",
    "                    return entity_configs['any_two_info']['citizenship_no_fathers_name']\n",
    "                elif has_citizenship and has_dob:\n",
    "                    return entity_configs['any_two_info']['citizenship_no_dob']\n",
    "                elif has_fathers and has_dob:\n",
    "                    return entity_configs['any_two_info']['fathers_name_dob']\n",
    "            elif criteria_count == 1:\n",
    "                if has_name:\n",
    "                    return entity_configs['one_info']['name']\n",
    "                elif has_citizenship:\n",
    "                    return entity_configs['one_info']['citizenship_no']\n",
    "                elif has_fathers:\n",
    "                    return entity_configs['one_info']['fathers_name']\n",
    "                elif has_dob:\n",
    "                    return entity_configs['one_info']['dob']\n",
    "        \n",
    "        # Fall back to equal weightage if no matching configuration\n",
    "        return {criterion: 100.0 / len(available_criteria) for criterion in available_criteria}\n",
    "\n",
    "    def _get_normalized_weights(self, record: Dict[str, Any], available_criteria: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get normalized weights based on entity type and available criteria\n",
    "        \n",
    "        Args:\n",
    "            record: Dictionary containing record data\n",
    "            available_criteria: List of available criteria fields\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping field names to normalized weights\n",
    "        \"\"\"\n",
    "        entity_type = self._determine_entity_type(record)\n",
    "        \n",
    "        print(f\"Entity type determined as: {entity_type}\")\n",
    "        print(f\"Available criteria: {available_criteria}\")\n",
    "        # Get weights based on entity type and available criteria\n",
    "        weights = self._get_weightage_config(entity_type, available_criteria)\n",
    "        print(f\"Normalized weights: {weights}\")\n",
    "        \n",
    "        # Normalize weights to sum to 1.0\n",
    "        total_weight = sum(weights.get(field, 0) for field in available_criteria)\n",
    "        \n",
    "        if total_weight == 0:\n",
    "            return {field: 1.0 / len(available_criteria) for field in available_criteria}\n",
    "            \n",
    "        return {field: weights.get(field, 0) / total_weight for field in available_criteria}\n",
    "\n",
    "    def match(self, cbs_data: pd.DataFrame, source_record: Dict[str, Any],\n",
    "          name_threshold: float = 0.6,  # Lowered from 0.7\n",
    "          final_threshold: float = 0.5,  # Lowered from 0.6\n",
    "          field_mapping: Optional[Dict[str, Tuple[str, str]]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform fuzzy matching between CBS data and a single source record with dynamic weightage\n",
    "        \n",
    "        Args:\n",
    "            cbs_data: DataFrame containing CBS data\n",
    "            source_record: Dictionary containing a single source record to match against\n",
    "            name_threshold: Minimum similarity threshold for name matching\n",
    "            final_threshold: Minimum similarity threshold for final matches\n",
    "            field_mapping: Optional dictionary mapping field types to (cbs_field, source_field) tuples\n",
    "                Example: {'name': ('ACCT_NAME', 'name'), 'pan_number': ('PAN_NUMBER', 'pan')}\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing matched records with similarity scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set default field mapping if not provided\n",
    "            if field_mapping is None:\n",
    "                field_mapping = {\n",
    "                    'name': ('ACCT_NAME', 'name'),\n",
    "                    'fathers_name': ('FATHERS_NAME', 'fathers_name'),\n",
    "                    'grandfathers_name': ('GRAND_FATHERS_NAME', 'grandfathers_name'),\n",
    "                    'citizenship_no': ('CTZ_NUMBER', 'citizenship_no'),\n",
    "                    'citizenship_issue_date': ('CTZ_ISSUE_DATE', 'issue_date'),\n",
    "                    'pan_number': ('PAN_NUMBER', 'company_pan_no'),\n",
    "                    'registration_number': ('REGISTRATION_NUMBER', 'company_registration_no'),\n",
    "                    'account_no': ('ACCT_NUMBER', 'account_no'),\n",
    "                    'nid': ('NID_NUMBER', 'nid'),\n",
    "                    'dob': ('CUST_DOB', 'dob'),\n",
    "                }\n",
    "            \n",
    "            # source_dict = source_record.to_dict() if isinstance(source_record, pd.Series) else source_record\n",
    "            source_dict = source_record\n",
    "            # Create a record with mapped fields\n",
    "            mapped_record = {}\n",
    "            for k, (_, source_field) in field_mapping.items():\n",
    "                if source_field in source_dict:\n",
    "                    mapped_record[k] = source_dict.get(source_field)\n",
    "\n",
    "            logger.info(f\"Mapped_records: {mapped_record}\")\n",
    "                \n",
    "            available_criteria = self._get_available_criteria(mapped_record)\n",
    "            logger.info(f\"Available criteria: {available_criteria}\")\n",
    "            \n",
    "            if not available_criteria:\n",
    "                logger.warning(f\"No matching criteria available for source record\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            weights = self._get_normalized_weights(mapped_record, available_criteria)\n",
    "            logger.info(f\"Normalized weights: {weights}\")\n",
    "            \n",
    "            # Only filter by name if available and meets minimum record count\n",
    "            filter_field = 'name' if 'name' in available_criteria else None\n",
    "            should_filter = filter_field \n",
    "\n",
    "            if should_filter:\n",
    "                filter_field_cbs, filter_field_source = field_mapping.get(filter_field, (None, None))\n",
    "                logger.info(f\"Filter_fields_CBS: {filter_field_cbs}\")\n",
    "                logger.info(f\"Filter_fields_Source: {filter_field_source}\")\n",
    "                \n",
    "                if filter_field_cbs and filter_field_source and filter_field_source in source_dict:\n",
    "                    # Calculate initial similarities\n",
    "                    initial_similarities = cbs_data.apply(\n",
    "                        lambda x: self._calculate_text_similarity(\n",
    "                            x.get(filter_field_cbs, ''), \n",
    "                            source_dict.get(filter_field_source, '')\n",
    "                        ),\n",
    "                        axis=1\n",
    "                    )     \n",
    "                    potential_matches = cbs_data[initial_similarities >= name_threshold].copy()\n",
    "                    logger.info(f\"Filtered from {len(cbs_data)} to {len(potential_matches)} records based on name similarity\")\n",
    "                else:\n",
    "                    potential_matches = cbs_data.copy()\n",
    "            else:\n",
    "                potential_matches = cbs_data.copy()\n",
    "            \n",
    "            if potential_matches.empty:\n",
    "                logger.warning(\"No potential matches found after initial filtering\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            similarity_scores = {}\n",
    "            \n",
    "            for criterion in available_criteria:\n",
    "                cbs_field, source_field = field_mapping.get(criterion, (None, None))\n",
    "                \n",
    "                if not cbs_field or not source_field or source_field not in source_dict:\n",
    "                    continue\n",
    "                    \n",
    "                source_value = source_dict.get(source_field, '')\n",
    "                                    \n",
    "                if criterion in ['citizenship_issue_date', 'dob']:\n",
    "                    similarity_scores[criterion] = potential_matches.apply(\n",
    "                        lambda x: self._calculate_date_similarity(\n",
    "                            x.get(cbs_field, ''),\n",
    "                            convert_bs_to_ad(source_value)\n",
    "                        ),\n",
    "                        axis=1\n",
    "                    )\n",
    "                else:\n",
    "                    similarity_scores[criterion] = potential_matches.apply(\n",
    "                        lambda x: self._calculate_text_similarity(\n",
    "                            x.get(cbs_field, ''), \n",
    "                            source_value\n",
    "                        ),\n",
    "                        axis=1\n",
    "                    )\n",
    "            \n",
    "            total_scores = pd.Series(0.0, index=potential_matches.index)\n",
    "            \n",
    "            for criterion in available_criteria:\n",
    "                if criterion in similarity_scores:\n",
    "                    total_scores += similarity_scores[criterion] * weights.get(criterion, 0)\n",
    "            \n",
    "            matches = potential_matches[total_scores >= final_threshold].copy()\n",
    "            \n",
    "            if matches.empty:\n",
    "                return\n",
    "\n",
    "            matches['total_score'] = total_scores[matches.index]\n",
    "            matches['match_status'] = 'Matched'\n",
    "            matches['criteria'] = str(weights)\n",
    "            \n",
    "            for criterion in available_criteria:\n",
    "                if criterion in similarity_scores:\n",
    "                    matches[f'{criterion}_score'] = similarity_scores[criterion][matches.index]\n",
    "            \n",
    "            matches = matches.sort_values('total_score', ascending=False)\n",
    "            return matches\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in fuzzy matching: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    def match_all_tickets(self, cbs_data: pd.DataFrame, source_data: Dict, ticket_id: Optional[str] = \"0\",\n",
    "                    name_threshold: float = 0.8,  \n",
    "                    final_threshold: float = 0.95, \n",
    "                    field_mapping: Optional[Dict[str, Tuple[str, str]]] = None,\n",
    "                    output_dir: str = 'matched_results'):\n",
    "        \"\"\"\n",
    "        Match a ticket in source data against CBS data and generate Excel file\n",
    "        \n",
    "        Args:\n",
    "            cbs_data: DataFrame containing CBS data\n",
    "            source_data: Dictionary containing source data (ticket)\n",
    "            ticket_id: Optional ticket ID\n",
    "            name_threshold: Minimum similarity threshold for name matching\n",
    "            final_threshold: Minimum similarity threshold for final matches\n",
    "            field_mapping: Optional dictionary mapping field types to (cbs_field, source_field) tuples\n",
    "            output_dir: Directory to save the generated Excel files\n",
    "            \n",
    "        Returns:\n",
    "            String containing the path to the generated Excel file\n",
    "        \"\"\"\n",
    "        matched_dir = os.path.join(output_dir, 'matched')\n",
    "        unmatched_dir = os.path.join(output_dir, 'unmatched')\n",
    "        \n",
    "        for directory in [output_dir, matched_dir, unmatched_dir]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "        \n",
    "        if 'entity_type' not in source_data.keys():\n",
    "            source_data['entity_type'] = self._determine_entity_type(source_data)\n",
    "        \n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Processing ticket {ticket_id}\")\n",
    "        source_record = source_data        \n",
    "        matches = self.match(\n",
    "            cbs_data, \n",
    "            source_record, \n",
    "            name_threshold, \n",
    "            final_threshold, \n",
    "            field_mapping\n",
    "        )\n",
    "        \n",
    "        ticket_name = f\"ticket_{ticket_id}\"\n",
    "        if 'name' in source_record and not pd.isna(source_record.get('name')) and source_record.get('name'):\n",
    "            ticket_name += f\"{source_record['name'].replace(' ', '')}\"\n",
    "        \n",
    "        entity_type = source_record.get('entity_type', self._determine_entity_type(source_record))\n",
    "        \n",
    "        mapped_source_record = {}\n",
    "        if field_mapping:\n",
    "            for k, (_, source_field) in field_mapping.items():\n",
    "                if source_field in source_record:\n",
    "                    mapped_source_record[k] = source_record.get(source_field)\n",
    "        \n",
    "        available_criteria = self._get_available_criteria(mapped_source_record)\n",
    "        \n",
    "        is_matched = not matches.empty and 'match_status' in matches.columns and any(matches['match_status'] == 'Matched')\n",
    "        output_subdir = matched_dir if is_matched else unmatched_dir\n",
    "        filename = os.path.join(output_subdir, f\"{ticket_name}.xlsx\")\n",
    "        \n",
    "        for col in source_data.keys():\n",
    "            if col in source_record:\n",
    "                matches[f'source_{col}'] = source_record[col]\n",
    "        \n",
    "        required_columns_order = [\n",
    "            'CIF_ID', 'source_entity_type', 'criteria', 'source_enforcement_request',\n",
    "            'ACCT_NAME', 'source_name', 'name_score',\n",
    "            'FATHERS_NAME', 'source_fathers_name', 'fathers_name_score', 'GRAND_FATHERS_NAME',\n",
    "            'source_grandfathers_name', 'CUST_DOB', 'source_dob', 'dob_score',\n",
    "            'CTZ_NUMBER', 'source_citizenship_no', 'citizenship_no_score', 'CTZ_ISSUE_DATE', 'source_issue_date',\n",
    "            'CTZ_ISSUED_DISTRICT', 'NID_NUMBER', 'source_pan_no', 'source_nid', 'ACCT_NUMBER',\n",
    "            'ACCT_STATUS', 'FREZ_CODE','source_account_no', 'account_no_score', 'source_company_name', 'company_name_score',\n",
    "            'source_company_pan_no', 'company_pan_no_score', 'total_score', 'action'\n",
    "        ]\n",
    "        \n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:                \n",
    "            match_status = \"Matched\" if is_matched else \"Unmatched\"\n",
    "            \n",
    "            if not matches.empty:\n",
    "                if 'criteria' in matches.columns:\n",
    "                    weights_criteria = matches['criteria'].iloc[0] if not matches.empty else \"\"\n",
    "                else:\n",
    "                    weights = self._get_normalized_weights(mapped_source_record, available_criteria)\n",
    "                    weights_criteria = str(weights)\n",
    "                \n",
    "                for col in required_columns_order:\n",
    "                    if col not in matches.columns:\n",
    "                        matches[col] = \"\"\n",
    "\n",
    "                matches = matches[required_columns_order]\n",
    "                matches = matches.dropna(axis=1, how='all')\n",
    "                matches.to_excel(writer, sheet_name='Matches', index=False)\n",
    "            else:\n",
    "                pd.DataFrame({'Message': ['No matches found']}).to_excel(writer, sheet_name='Matches', index=False)\n",
    "                weights = self._get_normalized_weights(mapped_source_record, available_criteria)\n",
    "                weights_criteria = str(weights)\n",
    "            \n",
    "            match_details = pd.DataFrame({\n",
    "                'Field': ['Total records processed', 'Number of matches found', 'Threshold used', \n",
    "                        'Entity type', 'Available criteria', 'Weights', 'Match status', 'Match date'],\n",
    "                'Value': [len(cbs_data), \n",
    "                        len(matches[matches['match_status'] == 'Matched']) if not matches.empty and 'match_status' in matches.columns else 0, \n",
    "                        final_threshold, \n",
    "                        entity_type,\n",
    "                        ', '.join(available_criteria),\n",
    "                        weights_criteria,\n",
    "                        match_status,\n",
    "                        datetime.now().strftime('%Y-%m-%d %H:%M:%S')]\n",
    "            })\n",
    "            match_details.to_excel(writer, sheet_name='Match Details', index=False)\n",
    "            \n",
    "            if not is_matched:\n",
    "                suggestions = pd.DataFrame({\n",
    "                    'Suggestion': [\n",
    "                        'Try lowering the threshold',\n",
    "                        'Check for typos in the name or other fields',\n",
    "                        'Verify if the person exists in the database',\n",
    "                        'Add more identification fields if available',\n",
    "                        'Consider manual verification'\n",
    "                    ]\n",
    "                })\n",
    "                suggestions.to_excel(writer, sheet_name='Suggestions', index=False)\n",
    "            \n",
    "            logger.info(f\"Saved results for ticket {ticket_id} to {filename}\")\n",
    "        \n",
    "        return match_status, filename\n",
    "\n",
    "def process_tickets_from_excel(cbs_data: pd.DataFrame, tickets_data: Dict, ticket_id: str,\n",
    "                             output_dir: str = 'matched_results',\n",
    "                             name_threshold: float = 0.9,\n",
    "                             final_threshold: float = 0.95):\n",
    "    \"\"\"\n",
    "    Process a ticket and generate matched results\n",
    "    \n",
    "    Args:\n",
    "        cbs_data: DataFrame containing CBS data\n",
    "        tickets_data: Dictionary containing ticket data\n",
    "        ticket_id: ID for the ticket\n",
    "        output_dir: Directory to save the generated Excel file\n",
    "        name_threshold: Minimum similarity threshold for name matching\n",
    "        final_threshold: Minimum similarity threshold for final matches\n",
    "        \n",
    "    Returns:\n",
    "        String containing the path to the generated Excel file\n",
    "    \"\"\"\n",
    "    field_mapping = {\n",
    "        'name': ('ACCT_NAME', 'name'),\n",
    "        'fathers_name': ('FATHERS_NAME', 'fathers_name'),\n",
    "        'grandfathers_name': ('GRAND_FATHERS_NAME', 'grandfathers_name'),\n",
    "        'citizenship_no': ('CTZ_NUMBER', 'citizenship_no'),\n",
    "        'citizenship_issue_date': ('CTZ_ISSUE_DATE', 'issue_date'),\n",
    "        'pan_number': ('PAN_NUMBER', 'company_pan_no'),\n",
    "        'registration_number': ('REGISTRATION_NUMBER', 'company_registration_no'),\n",
    "        'account_no': ('ACCT_NUMBER', 'account_no'),\n",
    "        'nid': ('NID_NUMBER', 'nid'),\n",
    "        'dob': ('CUST_DOB', 'dob'),\n",
    "    }\n",
    "    matcher = FuzzyMatcherComponent()\n",
    "    return matcher.match_all_tickets(\n",
    "        cbs_data, tickets_data, ticket_id, name_threshold, final_threshold, field_mapping, output_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KBL\\AppData\\Local\\Temp\\ipykernel_13064\\2347321047.py:5: DtypeWarning: Columns (11,12,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cbs_data = pd.read_csv(cbs_excel)\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"matched\"\n",
    "cbs_excel = r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\test\\cbs_full_view.csv'\n",
    "tickets_excel =r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\test\\letter_actions.xlsx'\n",
    "\n",
    "cbs_data = pd.read_csv(cbs_excel)\n",
    "tickets = pd.read_excel(tickets_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973-09-17 12:00:00\n"
     ]
    }
   ],
   "source": [
    "for i, data in cbs_data.iterrows():\n",
    "    data = data.to_dict()\n",
    "    print(pd.to_datetime(data.get('CTZ_ISSUE_DATE')))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity type determined as: individual\n",
      "Available criteria: ['name', 'fathers_name', 'citizenship_no']\n",
      "Entity type is individual\n",
      "Normalized weights: {'name': 10, 'citizenship_no': 80, 'fathers_name': 10, 'dob': 0}\n",
      "Entity type determined as: individual\n",
      "Available criteria: ['name', 'fathers_name', 'citizenship_no']\n",
      "Entity type is individual\n",
      "Normalized weights: {'name': 10, 'citizenship_no': 80, 'fathers_name': 10, 'dob': 0}\n",
      "Status: Unmatched\n",
      "Processed 48 tickets\n",
      "0:00:53.160086\n",
      "Entity type determined as: individual\n",
      "Available criteria: ['name', 'fathers_name', 'citizenship_no']\n",
      "Entity type is individual\n",
      "Normalized weights: {'name': 10, 'citizenship_no': 80, 'fathers_name': 10, 'dob': 0}\n",
      "Entity type determined as: individual\n",
      "Available criteria: ['name', 'fathers_name', 'citizenship_no']\n",
      "Entity type is individual\n",
      "Normalized weights: {'name': 10, 'citizenship_no': 80, 'fathers_name': 10, 'dob': 0}\n",
      "Status: Unmatched\n",
      "Processed 42 tickets\n",
      "0:00:52.626878\n",
      "Entity type determined as: individual\n",
      "Available criteria: ['name', 'fathers_name', 'citizenship_no']\n",
      "Entity type is individual\n",
      "Normalized weights: {'name': 10, 'citizenship_no': 80, 'fathers_name': 10, 'dob': 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m start_time = datetime.now()\n\u001b[32m      4\u001b[39m ticket = ticket.to_dict()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m matched_status, results = \u001b[43mprocess_tickets_from_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcbs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatched_status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tickets\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 688\u001b[39m, in \u001b[36mprocess_tickets_from_excel\u001b[39m\u001b[34m(cbs_data, tickets_data, ticket_id, output_dir, name_threshold, final_threshold)\u001b[39m\n\u001b[32m    675\u001b[39m field_mapping = {\n\u001b[32m    676\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: (\u001b[33m'\u001b[39m\u001b[33mACCT_NAME\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    677\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfathers_name\u001b[39m\u001b[33m'\u001b[39m: (\u001b[33m'\u001b[39m\u001b[33mFATHERS_NAME\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfathers_name\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    685\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdob\u001b[39m\u001b[33m'\u001b[39m: (\u001b[33m'\u001b[39m\u001b[33mCUST_DOB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdob\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    686\u001b[39m }\n\u001b[32m    687\u001b[39m matcher = FuzzyMatcherComponent()\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch_all_tickets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcbs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtickets_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticket_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 564\u001b[39m, in \u001b[36mFuzzyMatcherComponent.match_all_tickets\u001b[39m\u001b[34m(self, cbs_data, source_data, ticket_id, name_threshold, final_threshold, field_mapping, output_dir)\u001b[39m\n\u001b[32m    562\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing ticket \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticket_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    563\u001b[39m source_record = source_data        \n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m matches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcbs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource_record\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_mapping\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m ticket_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mticket_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticket_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m source_record \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd.isna(source_record.get(\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m source_record.get(\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 424\u001b[39m, in \u001b[36mFuzzyMatcherComponent.match\u001b[39m\u001b[34m(self, cbs_data, source_record, name_threshold, final_threshold, field_mapping)\u001b[39m\n\u001b[32m    420\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFilter_fields_Source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilter_field_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filter_field_cbs \u001b[38;5;129;01mand\u001b[39;00m filter_field_source \u001b[38;5;129;01mand\u001b[39;00m filter_field_source \u001b[38;5;129;01min\u001b[39;00m source_dict:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# Calculate initial similarities\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     initial_similarities = \u001b[43mcbs_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_text_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_field_cbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_field_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m     \n\u001b[32m    431\u001b[39m     potential_matches = cbs_data[initial_similarities >= name_threshold].copy()\n\u001b[32m    432\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFiltered from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cbs_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(potential_matches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records based on name similarity\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KBL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KBL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KBL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KBL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 425\u001b[39m, in \u001b[36mFuzzyMatcherComponent.match.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    420\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFilter_fields_Source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilter_field_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filter_field_cbs \u001b[38;5;129;01mand\u001b[39;00m filter_field_source \u001b[38;5;129;01mand\u001b[39;00m filter_field_source \u001b[38;5;129;01min\u001b[39;00m source_dict:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# Calculate initial similarities\u001b[39;00m\n\u001b[32m    424\u001b[39m     initial_similarities = cbs_data.apply(\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_text_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_field_cbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_field_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    429\u001b[39m         axis=\u001b[32m1\u001b[39m\n\u001b[32m    430\u001b[39m     )     \n\u001b[32m    431\u001b[39m     potential_matches = cbs_data[initial_similarities >= name_threshold].copy()\n\u001b[32m    432\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFiltered from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cbs_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(potential_matches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records based on name similarity\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mFuzzyMatcherComponent._calculate_text_similarity\u001b[39m\u001b[34m(self, text1, text2)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    177\u001b[39m text1 = \u001b[38;5;28mself\u001b[39m._preprocess_text(text1)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m text2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text1 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text2:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 169\u001b[39m, in \u001b[36mFuzzyMatcherComponent._preprocess_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    167\u001b[39m text = \u001b[38;5;28mstr\u001b[39m(text).lower().strip()\n\u001b[32m    168\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m text = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43ms+\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KBL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\re\\__init__.py:179\u001b[39m, in \u001b[36msub\u001b[39m\u001b[34m(pattern, repl, string, count, flags)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags).search(string)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msub\u001b[39m(pattern, repl, string, count=\u001b[32m0\u001b[39m, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    180\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags).sub(repl, string, count)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "for i, ticket in tickets.iterrows():\n",
    "    start_time = datetime.now()\n",
    "    ticket = ticket.to_dict()\n",
    "    matched_status, results = process_tickets_from_excel(\n",
    "            cbs_data, ticket, output_dir, str(i), name_threshold=0.9, final_threshold=0.95\n",
    "        )\n",
    "    print(f\"Status: {matched_status}\")\n",
    "    print(f\"Processed {len(results)} tickets\")\n",
    "    print(datetime.now() - start_time)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name name\n",
      "fathers_name fathers_name\n",
      "grandfathers_name grandfathers_name\n",
      "citizenship_no citizenship_no\n",
      "citizenship_issue_date issue_date\n",
      "pan_number company_pan_no\n",
      "registration_number company_registration_no\n",
      "account_no account_no\n",
      "nid nid\n"
     ]
    }
   ],
   "source": [
    "field_mapping = {\n",
    "    'name': ('ACCT_NAME', 'name'),\n",
    "    'fathers_name': ('FATHERS_NAME', 'fathers_name'),\n",
    "    'grandfathers_name': ('GRAND_FATHERS_NAME', 'grandfathers_name'),\n",
    "    'citizenship_no': ('CTZ_NUMBER', 'citizenship_no'),\n",
    "    'citizenship_issue_date': ('CTZ_ISSUE_DATE', 'issue_date'),\n",
    "    'pan_number': ('PAN_NUMBER', 'company_pan_no'),\n",
    "    'registration_number': ('REGISTRATION_NUMBER', 'company_registration_no'),\n",
    "    'account_no': ('ACCT_NUMBER', 'account_no'),\n",
    "    'nid': ('NID_NUMBER', 'nid'),\n",
    "}\n",
    "\n",
    "for k, (_, source_field) in field_mapping.items():\n",
    "    print(k, source_field)\n",
    "    # if source_field in source_dict:\n",
    "    #     mapped_record[k] = source_dict.get(source_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('name', ('ACCT_NAME', 'name')), ('fathers_name', ('FATHERS_NAME', 'fathers_name')), ('grandfathers_name', ('GRAND_FATHERS_NAME', 'grandfathers_name')), ('citizenship_no', ('CTZ_NUMBER', 'citizenship_no')), ('citizenship_issue_date', ('CTZ_ISSUE_DATE', 'issue_date')), ('pan_number', ('PAN_NUMBER', 'company_pan_no', 'pan_no')), ('registration_number', ('REGISTRATION_NUMBER', 'company_registration_no')), ('account_no', ('ACCT_NUMBER', 'account_no')), ('nid', ('NID_NUMBER', 'nid'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_mapping.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"matched\"\n",
    "cbs_excel = r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\test\\cbs_view.xlsx'\n",
    "tickets_excel =r'C:\\Users\\KBL\\Desktop\\LetterAction_Bot1\\kbl-letter-action\\test\\letter_actions.xlsx'\n",
    "results = process_tickets_from_excel(\n",
    "        cbs_excel, tickets_excel, output_dir, name_threshold=0.7, final_threshold=0.6\n",
    "    )\n",
    "print(f\"Processed {len(results)} tickets\")\n",
    "matched_count = sum(1 for df in results.values() if not df.empty)\n",
    "print(f\"Found matches for {matched_count} tickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'letter_head', 'chalani_no', 'letter_date', 'institution',\n",
       "       'enforcement_request', 'suspicious_activity', 'entity_type', 'name',\n",
       "       'account_no', 'address', 'citizenship_no', 'fathers_name',\n",
       "       'grandfathers_name', 'issue_date', 'company_name', 'company_address',\n",
       "       'company_pan_no', 'company_registration_no', 'company_account_no'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_dir = r'c:\\Users\\KBL\\Downloads\\797--20241219044306.xlsx'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(excel_dir)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
